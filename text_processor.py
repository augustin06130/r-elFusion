import re
from nltk.tokenize import sent_tokenize
import nltk

def setup_nltk():
    """TÃ©lÃ©charge les ressources NLTK nÃ©cessaires"""
    try:
        nltk.download('punkt')
        try:
            nltk.download('punkt_tab')
        except Exception as e:
            print(f"âš ï¸ Avertissement lors du tÃ©lÃ©chargement punkt_tab: {e}")
            print("Tentative de continuer malgrÃ© l'erreur...")
    except Exception as e:
        print(f"âš ï¸ Erreur lors du tÃ©lÃ©chargement des ressources NLTK: {e}")

def clean_reddit_text(raw_text):
    """
    Nettoie le texte brut d'un post Reddit
    """
    print("ğŸ§¼ Nettoyage du texte...")
    # Supprimer les liens
    text = re.sub(r'\[.*?\]\(.*?\)', '', raw_text)  # markdown [xxx](url)
    text = re.sub(r'http\S+', '', text)             # liens bruts
    # Supprimer les mentions "Edit:", "TLDR", etc.
    text = re.sub(r'(?i)(edit|update|tl;dr|tldr):?.*$', '', text, flags=re.MULTILINE)
    # Supprimer les caractÃ¨res spÃ©ciaux inutiles
    text = re.sub(r'\*+', '', text)
    # Supprimer les lignes vides multiples
    text = re.sub(r'\n\s*\n', '\n\n', text)
    return text.strip()

def split_text_into_chunks(text, words_per_chunk=200):
    """
    DÃ©coupe le texte en segments avec une fin logique
    """
    print(f"âœ‚ï¸ DÃ©coupage du texte en segments d'environ {words_per_chunk} mots...")

    try:
        sentences = sent_tokenize(text)
    except LookupError:
        # Si les ressources ne sont pas dÃ©jÃ  tÃ©lÃ©chargÃ©es
        setup_nltk()
        sentences = sent_tokenize(text)

    chunks = []
    current_chunk = ""
    word_count = 0

    for sentence in sentences:
        sentence_word_count = len(sentence.split())
        if word_count + sentence_word_count > words_per_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
            word_count = sentence_word_count
        else:
            current_chunk += sentence + " "
            word_count += sentence_word_count

    if current_chunk:
        chunks.append(current_chunk.strip())

    print(f"âœ… {len(chunks)} segments crÃ©Ã©s")
    return chunks
